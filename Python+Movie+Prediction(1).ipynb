{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict movie offer suggestion using IBM Watson Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces commands for getting data and for basic data cleaning and exploration, pipeline creation, model training, model persistance to Watson Machine Learning repository, model deployment, and scoring.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 2.0 and Apache速 Spark 2.0.\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "-  Load a CSV file into an Apache速 Spark DataFrame.\n",
    "-  Explore data.\n",
    "-  Prepare data for training and evaluation.\n",
    "-  Create an Apache速 Spark machine learning pipeline.\n",
    "-  Train and evaluate a model.\n",
    "-  Persist a pipeline and model in Watson Machine Learning repository.\n",
    "-  Deploy a model for online scoring using Wastson Machine Learning API.\n",
    "-  Score sample scoring data using the Watson Machine Learning API.\n",
    "\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Setup](#setup)\n",
    "2.\t[Load and explore data](#load)\n",
    "3.\t[Create spark ml model](#model)\n",
    "4.\t[Persist model](#save)\n",
    "5.\t[Predict locally and visualize](#predict)\n",
    "6.\t[Deploy and score in a Cloud](#deploy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a [Watson Machine Learning Service](https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/) instance (a free plan is offered). \n",
    "-  Upload **movie.csv** data as a data asset in IBM Data Science Experience.\n",
    "-  Make sure that you are using a Spark 2.0 kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 2.  Load and explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM Data Science Experience (DSX) makes it easy to load your files with a few clicks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: Import the data and add .option('inferSchema','true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'auth_url': 'https://identity.open.softlayer.com',\n",
    "    'project_id': '1d01a4b956544ae193baced39418dffc',\n",
    "    'region': 'dallas',\n",
    "    'user_id': '284703554b9c4314be591d5858cb32bb',\n",
    "    'username': 'member_d42cb5e9f21e150269239deba3605b754a22cff0',\n",
    "    'password': 'ao*9J2?vmvkO(HYe'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_fd0d395ea9634e6ba8708dc2eebb3e12_configs'\n",
    "bmos = ibmos2spark.bluemix(sc, credentials, configuration_name)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_data_3 = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(bmos.url('MachineLearningProject', 'movies.csv'))\n",
    "df_data_3.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the loaded data by using the following Apache速 Spark DataFrame methods:\n",
    "-  print schema\n",
    "-  count all the records\n",
    "-  print top five records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df_data_3\n",
    "\n",
    "df.printSchema()\n",
    "print \"# of records: \" + str(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 1165 rows and we have 7 fields we will use to predict the title (label) of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 3. Create an Apache Spark machine learning model\n",
    "\n",
    "In this section we will prepare data, create an Apache Spark machine learning pipeline, and train a model.\n",
    "\n",
    "\n",
    "### 3.1:  Prepare data\n",
    "\n",
    "In this subsection we will split our data into: training, test, and predict datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_data = df.randomSplit([0.7, 0.28, 0.02], 24)\n",
    "\n",
    "training_data = split_data[0]\n",
    "test_data = split_data[1]\n",
    "predict_data = split_data[2]\n",
    "\n",
    "print \"Training records: \" + str(training_data.count())\n",
    "print \"Test records: \" + str(test_data.count())\n",
    "print \"Prediction records: \" + str(predict_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our data has been successfully split into three datasets: \n",
    "\n",
    "-  The training dataset, which is the largest group, is used for training.\n",
    "-  The test dataset will be used for model evaluation and is used to test the assumptions of the model.\n",
    "-  The predict dataset will be used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2:  Create pipeline and train a model\n",
    "\n",
    "In this section we create an Apache Spark machine learning pipeline and then train the model.\n",
    "\n",
    "First we need to import several packages that will be used in the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First we need to convert all the string fields to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringIndexer_labels = StringIndexer(inputCol=\"title\", outputCol=\"label\").fit(df)\n",
    "stringIndexer_gender = StringIndexer(inputCol=\"GENDER\", outputCol=\"GENDER_IX\").fit(df)\n",
    "stringIndexer_paymentmethod = StringIndexer(inputCol=\"PAYMENTMETHOD\", outputCol=\"PAYMENTMETHOD_IX\").fit(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feature vector by combining all features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorAssembler_features = VectorAssembler(inputCols=[\"GENDER_IX\",\"SENIORCITIZEN\",\"DEPENDENTS\",\"TENURE\",\"PAPERLESSBILLING\",\n",
    "                                                     \"PAYMENTMETHOD_IX\",\"MONTHLYCHARGES\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a Random Forest estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert the indexed labels back to the original label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=stringIndexer_labels.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will put all the steps into a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=[stringIndexer_labels,stringIndexer_gender, stringIndexer_paymentmethod,\n",
    "                               vectorAssembler_features, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a model using our pipeline and the training_data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_rf = pipeline_rf.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check our model accuracy using our test_data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model_rf.transform(test_data)\n",
    "evaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluatorRF.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we would tune the model for desired accuracy, for this example we will move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "## 4. Persist model in IBM Watson Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn how to store your pipeline and model in Watson Machine Learning repository by using python client libraries.\n",
    "\n",
    "First, you must import client libraries.\n",
    "\n",
    "**Note**: Apache Spark 2.0 or higher is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from repository.mlrepositoryclient import MLRepositoryClient\n",
    "from repository.mlrepositoryartifact import MLRepositoryArtifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to Watson Machine Learning service on Bluemix.\n",
    "\n",
    "**Action**: Use your Watson Machine Learning service instance credentials below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "username = '<from your watson ml service>'\n",
    "password = '<from your watson ml service>'\n",
    "service_path = '<from your watson ml service>'\n",
    "instance_id = '<from your watson ml service>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: service_path, user and password can be found on **Service Credentials** tab of service instance created in Bluemix. If you cannot see **instance_id** field in **Serice Credentials** generate new credentials by pressing **New credential (+)** button. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_repository_client = MLRepositoryClient(service_path)\n",
    "ml_repository_client.authorize(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model artifact (abstraction layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_artifact = MLRepositoryArtifact(model_rf, training_data=training_data, name=\"Movie Prediction with Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Save pipeline and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_model = ml_repository_client.models.save(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get saved model metadata from Watson Machine Learning using the meta.available_props() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_model.meta.available_props()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**:  **modelVersionHref** is our model unique id in Watson Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print saved_model.meta.prop(\"modelVersionHref\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Load model\n",
    "\n",
    "Now that we saved the model we will load it and verify the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print str(loadedModelArtifact.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"predict\"></a>\n",
    "## 5. Predict locally and visualize\n",
    "\n",
    "In this section we will score test data using the loaded model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Make local prediction using loaded model and predict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = loadedModelArtifact.model_instance().transform(predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.select(\"predictedLabel\").groupBy(\"predictedLabel\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "## 6. Deploy and create online scoring endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn how to create online scoring and to score a new data record by using the Watson Machine Learning REST API. \n",
    "For more information about REST APIs, see the [Swagger Documentation](http://watson-ml-api.mybluemix.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the Watson Machine Leraning REST API you must generate an access token. To do that you can use the following sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib3, requests, json\n",
    "\n",
    "headers = urllib3.util.make_headers(basic_auth='{}:{}'.format(username, password))\n",
    "url = '{}/v3/identity/token'.format(service_path)\n",
    "response = requests.get(url, headers=headers)\n",
    "mltoken = json.loads(response.text).get('token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have the token we can create an online scoring endpoint.\n",
    "\n",
    "First we will check the model for existing deployments and get the deployments url, then we will create the online deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "published_model_details = service_path + \"/v3/wml_instances/\" + instance_id + \"/published_models/\"\\\n",
    "+ loadedModelArtifact.uid \n",
    "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n",
    "\n",
    "response_get_model_details = requests.get(published_model_details, headers=header)\n",
    "\n",
    "print 'Existing deployment count: ' + str(json.loads(response_get_model_details.text).get('entity').get('deployments').get('count'))\n",
    "deployments_endpoint = json.loads(response_get_model_details.text).get('entity').get('deployments').get('url')\n",
    "print deployments_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "payload_online_endpoint = {\"name\": \"Movie Prediction Deployment\", \"description\": \"Movie prediction endpoint\\\n",
    "for suggesting movies to customers.\", \"type\": \"online\"}\n",
    "response_online = requests.post(deployments_endpoint, json=payload_online_endpoint, headers=header)\n",
    "\n",
    "scoring_endpoint = json.loads(response_online.text).get('entity').get('scoring_url')\n",
    "print scoring_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can send (POST) a new scoring request to our deployed model to get a movie prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "payload_scoring = {\"fields\": [\"GENDER\",\"SENIORCITIZEN\",\"DEPENDENTS\",\"TENURE\",\"PAPERLESSBILLING\",\\\n",
    "                             \"PAYMENTMETHOD\", \"MONTHLYCHARGES\"],\"values\": [[\"Male\",0,0,25,1,\"Credit card (automatic)\",79.95]]}\n",
    "response_scoring = requests.post(scoring_endpoint, json=payload_scoring, headers=header)\n",
    "\n",
    "print json.loads(response_scoring.text)[\"values\"][0]\\\n",
    "[len(json.loads(response_scoring.text)[\"values\"][0])-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we have a working online endpoint to use in our call center application**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
