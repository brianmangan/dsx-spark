{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL4.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL2.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL3.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL1.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL queries Dataframes, not RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data file on world banks will downloaded from GitHub after removing any previous data that may exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In jupyter notebooks you can prefice commands with a ! to run shell commands\n",
    "# here we remove any files with the name of the file we are going to download\n",
    "# then download the file\n",
    "\n",
    "!rm world_bank.json.gz -f\n",
    "!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many other types are supported including text and Parquet\n",
    "\n",
    "Here we are creating a Dataframe, similar to an RDD, but with a schema and abstraction that allows\n",
    "for SQL to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#You can load json, text and other files using sqlContext\n",
    "#unlinke an RDD, this will attempt to create a schema around the data\n",
    "#self describing data works really well for this\n",
    "\n",
    "example1_df = spark.read.json(\"./world_bank.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark SQL has the ability to infer the schema of JSON data and understand the structure of the data\n",
    "#once we have created the Dataframe, we can print out the schema to see the shape of the data\n",
    "\n",
    "print example1_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's take a look at the first two rows of data\n",
    "\n",
    "The example below enumerates our \"take\" command that pulls 2 items from the Dataframe\n",
    "<br>a simpiler option to see the data could also be:<br>\n",
    "\n",
    "##### copy and run the following code\n",
    "    for row in example1_df.take(2):\n",
    "        print row\n",
    "        print \"*\" * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in example1_df.take(2):\n",
    "        print row\n",
    "        print \"*\" * 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's register a table which is a pointer to the Dataframe and allows data access via Spark SQL\n",
    "\n",
    "##### copy and run the following code\n",
    "    #Simply use the Dataframe Object to create the table:\n",
    "    example1_df.registerTempTable(\"world_bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #Simply use the Dataframe Object to create the table:\n",
    "    example1_df.registerTempTable(\"world_bank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The returned object will be a dataframe\n",
    "##### copy and run the following code\n",
    "    temp_df =  spark.sql(\"select * from world_bank limit 2\")\n",
    "\n",
    "    print type(temp_df)\n",
    "    print \"*\" * 20\n",
    "    print temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    temp_df =  spark.sql(\"select * from world_bank limit 2\")\n",
    "\n",
    "    print type(temp_df)\n",
    "    print \"*\" * 20\n",
    "    print temp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One nice feature of the notebooks and python is that we can show it in a table via Pandas\n",
    "spark.sql(\"select id, borrower from world_bank limit 2\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select id, borrower from world_bank limit 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a simple group by example:\n",
    "\n",
    "#### Count the number of projects by each country, only list the top 10\n",
    "\n",
    "##### Copy and paste the following: \n",
    "\n",
    "    query = \"\"\"\n",
    "    select countryname,\n",
    "    count(1) as projects\n",
    "    from world_bank\n",
    "    group by countryname\n",
    "    order by projects desc\n",
    "    limit 10\n",
    "    \"\"\"\n",
    "    spark.sql(query).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    query = \"\"\"\n",
    "    select countryname,\n",
    "    count(1) as projects\n",
    "    from world_bank\n",
    "    group by countryname\n",
    "    order by projects desc\n",
    "    limit 10\n",
    "    \"\"\"\n",
    "    spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example of Adding a Schema (headers) to an RDD and using it as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the example below a simple RDD is created with Random Data in two columns and an ID column.\n",
    "\n",
    "#### copy and run the following code\n",
    "\n",
    "    import random\n",
    "\n",
    "    #first let's create a simple RDD\n",
    "\n",
    "    #create a Python list of lists for our example\n",
    "    data_e2 = []\n",
    "    for x in range(1,6):\n",
    "        random_int = int(random.random() * 10)\n",
    "        data_e2.append([x, random_int, random_int^2])\n",
    "\n",
    "    #create the RDD with the random list of lists\n",
    "    rdd_example2 = sc.parallelize(data_e2)\n",
    "    print rdd_example2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#first let's create a simple RDD\n",
    "\n",
    "#create a Python list of lists for our example\n",
    "data_e2 = []\n",
    "for x in range(1,6):\n",
    "    random_int = int(random.random() * 10)\n",
    "    data_e2.append([x, random_int, random_int^2])\n",
    "\n",
    "#create the RDD with the random list of lists\n",
    "rdd_example2 = sc.parallelize(data_e2)\n",
    "print rdd_example2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can assign some header information\n",
    "\n",
    "#### copy and run the following code\n",
    "    from pyspark.sql.types import *\n",
    "\n",
    "    # The schema is encoded in a string.\n",
    "    schemaString = \"ID VAL1 VAL2\"\n",
    "\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "    schema = StructType(fields)\n",
    "\n",
    "    # Apply the schema to the RDD.\n",
    "    schemaExample = sqlContext.createDataFrame(rdd_example2, schema)\n",
    "\n",
    "    # Register the DataFrame as a table.\n",
    "    schemaExample.registerTempTable(\"example2\")\n",
    "\n",
    "    # Pull the data\n",
    "    print schemaExample.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from pyspark.sql.types import *\n",
    "\n",
    "    # The schema is encoded in a string.\n",
    "    schemaString = \"ID VAL1 VAL2\"\n",
    "\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "    schema = StructType(fields)\n",
    "\n",
    "    # Apply the schema to the RDD.\n",
    "    schemaExample = sqlContext.createDataFrame(rdd_example2, schema)\n",
    "\n",
    "    # Register the DataFrame as a table.\n",
    "    schemaExample.registerTempTable(\"example2\")\n",
    "\n",
    "    # Pull the data\n",
    "    print schemaExample.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can convert rdd_example3 to a Dataframe\n",
    "\n",
    "##### copy and run this code\n",
    "    from pyspark.sql import Row\n",
    "\n",
    "    rdd_example3 = rdd_example2.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2]))\n",
    "    print rdd_example3.collect()\n",
    "    df_example3 = rdd_example3.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from pyspark.sql import Row\n",
    "\n",
    "    rdd_example3 = rdd_example2.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2]))\n",
    "    print rdd_example3.collect()\n",
    "    df_example3 = rdd_example3.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register this new data frame as a table \n",
    "register as temp table, call it 'df_example3'\n",
    "\n",
    "    df_example3.registerTempTable('df_example3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_example3.registerTempTable('df_example3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another powerful feature is the ability to create Functions and Use them in SQL Here is a simple example\n",
    "\n",
    "First we create a function in Python, then register it allowing for us to call it via SQL\n",
    "\n",
    "#### copy and run the following code\n",
    "    def simple_function(v):\n",
    "        return int(v * 10)\n",
    "\n",
    "    #test the function\n",
    "    print simple_function(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def simple_function(v):\n",
    "        return int(v * 10)\n",
    "\n",
    "    #test the function\n",
    "    print simple_function(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can register the function for use in SQL\n",
    "spark.udf.register('simple_function',simple_function,pyspark.sql.types.IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.udf.register('simple_function',simple_function,pyspark.sql.types.IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAL1 and VAL2 look like strings, we can cast them as well\n",
    "\n",
    "    query = \"\"\"\n",
    "    select\n",
    "        ID,\n",
    "        VAL1,\n",
    "        VAL2,\n",
    "        simple_function(cast(VAL1 as int)) as s_VAL1,\n",
    "        simple_function(cast(VAL2 as int)) as s_VAL2\n",
    "    from\n",
    "     example2\n",
    "    \"\"\"\n",
    "    spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    query = \"\"\"\n",
    "    select\n",
    "        ID,\n",
    "        VAL1,\n",
    "        VAL2,\n",
    "        simple_function(cast(VAL1 as int)) as s_VAL1,\n",
    "        simple_function(cast(VAL2 as int)) as s_VAL2\n",
    "    from\n",
    "     example2\n",
    "    \"\"\"\n",
    "    spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas & Seaborn Example\n",
    "Pandas & Seaborn are a common abstraction for working with data in Python.\n",
    "\n",
    "We can turn Pandas Dataframes into Spark Dataframes, the advantage of this \n",
    "could be scale or allowing us to run SQL statements agains the data.\n",
    "\n",
    "### copy and run the following code\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    print pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    print pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's grab some UFO data to play with\n",
    "\n",
    "    !rm SIGHTINGS.csv -f\n",
    "    !wget https://raw.githubusercontent.com/brianmangan/dsx-spark/master/SIGHTINGS.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    !rm SIGHTINGS.csv -f\n",
    "    !wget https://raw.githubusercontent.com/brianmangan/dsx-spark/master/SIGHTINGS.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the CSV file, we can create a Pandas Dataframe:\n",
    "    pandas_df = spark.read.format(\"csv\").options(header=\"true\").load(\"./SIGHTINGS.csv\")\n",
    "    pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    pandas_df = spark.read.format(\"csv\").options(header=\"true\").load(\"./SIGHTINGS.csv\")\n",
    "    pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a Temp Table & look at the dataset, query on date posted and get count \n",
    "#### copy and paste this code\n",
    "\n",
    "    ufo_data = pandas_df\n",
    "    ufo_data.registerTempTable('ufo_data')\n",
    "    \n",
    "#### In another cell:\n",
    "\n",
    "\n",
    "    query = \"\"\"\n",
    "    select count(*), date_posted \n",
    "    FROM\n",
    "        ufo_data\n",
    "    GROUP BY \n",
    "        date_posted\n",
    "    limit 20\n",
    "\n",
    "    \"\"\"\n",
    "    spark.sql(query).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ufo_data = pandas_df\n",
    "    ufo_data.registerTempTable('ufo_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    query = \"\"\"\n",
    "    select count(*), date_posted \n",
    "    FROM\n",
    "        ufo_data\n",
    "    GROUP BY \n",
    "        date_posted\n",
    "    limit 20\n",
    "\n",
    "    \"\"\"\n",
    "    spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Data\n",
    "- Here are some simple ways to create charts using Pandas and Seaborn\n",
    "- In order to display in the notebook we need to tell matplotlib to render inline\n",
    "at this point import the supporting libraries as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can call a function \"plot\" to create the charts.\n",
    "Since most charts are created from aggregates the record\n",
    "set should be small enough to store in Pandas\n",
    "\n",
    "We can take our UFO data from before and create a \n",
    "Pandas Dataframe from the Spark Dataframe\n",
    "\n",
    "    ufos_df = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ufos_df = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot we call the \"plot\" method and specify the type, x and y axis columns\n",
    "and optionally the size of the chart.\n",
    "\n",
    "Many more details can be found here:\n",
    "http://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "    \n",
    "    \n",
    "#### copy and run this code\n",
    "    ufos_df.plot(kind='bar', x='date_posted', y='count(1)', figsize=(6, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufos_df.plot(kind='bar', x='date_posted', y='count(1)', figsize=(6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many observations have been made across the entire dataset\n",
    "    spark.sql(\"select count(*) from ufo_data\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from ufo_data\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get a description and some info about your dataframe \n",
    "    ufo_pandas = pandas_df.toPandas()\n",
    "    ufo_pandas.describe()\n",
    "\n",
    "#### In another Cell:\n",
    "    ufo_pandas.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ufo_pandas = pandas_df.toPandas()\n",
    "    ufo_pandas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ufo_pandas.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plot\n",
    "Using Seaborn for collecting sightings by year, we can see that the peak of sightings occured around the year 2000.\n",
    "\n",
    "    The steps we took:\n",
    "        Using the pandas dataframe we cleanse the datatype and make it integers for visualization.\n",
    "\n",
    "    ufo_pandas['datetime'] = pd.to_datetime(ufo_pandas['datetime'], errors='coerce')\n",
    "    ufo_pandas['year'] = ufo_pandas['datetime'].dt.year\n",
    "    ufo_pandas['year'] = ufo_pandas['year'].fillna(0).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.distplot(ufo_pandas['year'])\n",
    "    plt.xlim(1900,2015)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    ufo_pandas['datetime'] = pd.to_datetime(ufo_pandas['datetime'], errors='coerce')\n",
    "    ufo_pandas['year'] = ufo_pandas['datetime'].dt.year\n",
    "    ufo_pandas['year'] = ufo_pandas['year'].fillna(0).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.distplot(ufo_pandas['year'])\n",
    "    plt.xlim(1900,2015)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sightings by State\n",
    "    From this bar chart, we see that there are more than 500 UFO sightings reported in California. And there are 6 states that have reported more than 200 UFO sightings.\n",
    "    \n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    sns.countplot(x=\"state\", data=ufo_pandas, palette=\"Greens_d\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.countplot(x=\"state\", data=ufo_pandas, palette=\"Greens_d\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
