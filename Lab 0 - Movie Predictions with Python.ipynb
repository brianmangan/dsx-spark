{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 2 with Spark 2.0", "name": "python2-spark20"}, "language_info": {"version": "2.7.11", "nbconvert_exporter": "python", "codemirror_mode": {"version": 2, "name": "ipython"}, "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython2", "file_extension": ".py"}}, "nbformat_minor": 1, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Predict movie offer suggestion using IBM Watson Machine Learning"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook introduces commands for getting data and for basic data cleaning and exploration, pipeline creation, model training, model persistance to Watson Machine Learning repository, model deployment, and scoring.\n\nSome familiarity with Python is helpful. This notebook uses Python 2.0 and Apache\u00ae Spark 2.0.\n\n\n## Learning goals\n\nThe learning goals of this notebook are:\n\n-  Load a CSV file into an Apache\u00ae Spark DataFrame.\n-  Explore data.\n-  Prepare data for training and evaluation.\n-  Create an Apache\u00ae Spark machine learning pipeline.\n-  Train and evaluate a model.\n-  Persist a pipeline and model in Watson Machine Learning repository.\n-  Deploy a model for online scoring using Wastson Machine Learning API.\n-  Score sample scoring data using the Watson Machine Learning API.\n\n\n\n## Contents\n\nThis notebook contains the following parts:\n\n1.\t[Setup](#setup)\n2.\t[Load and explore data](#load)\n3.\t[Create spark ml model](#model)\n4.\t[Persist model](#save)\n5.\t[Predict locally and visualize](#predict)\n6.\t[Deploy and score in a Cloud](#deploy)\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## 1. Setup\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a [Watson Machine Learning Service](https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/) instance (a free plan is offered). \n-  Upload **movie.csv** data as a data asset in IBM Data Science Experience.\n-  Make sure that you are using a Spark 2.0 kernel.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load\"></a>\n## 2.  Load and explore data"}, {"metadata": {}, "cell_type": "markdown", "source": "IBM Data Science Experience (DSX) makes it easy to load your files with a few clicks!"}, {"metadata": {}, "cell_type": "markdown", "source": "**Action**: Import the data and add .option('inferSchema','true)"}, {"execution_count": 31, "outputs": [{"output_type": "execute_result", "metadata": {}, "data": {"text/plain": "[Row(title=u'Rounders', GENDER=u'Female', SENIORCITIZEN=0, DEPENDENTS=1, TENURE=39, PAPERLESSBILLING=1, PAYMENTMETHOD=u'Credit card (automatic)', MONTHLYCHARGES=99.95),\n Row(title=u'Rounders', GENDER=u'Female', SENIORCITIZEN=0, DEPENDENTS=1, TENURE=71, PAPERLESSBILLING=0, PAYMENTMETHOD=u'Bank transfer (automatic)', MONTHLYCHARGES=20.1),\n Row(title=u'Rounders', GENDER=u'Female', SENIORCITIZEN=0, DEPENDENTS=1, TENURE=72, PAPERLESSBILLING=1, PAYMENTMETHOD=u'Electronic check', MONTHLYCHARGES=78.95),\n Row(title=u'Payback', GENDER=u'Male', SENIORCITIZEN=0, DEPENDENTS=0, TENURE=12, PAPERLESSBILLING=0, PAYMENTMETHOD=u'Bank transfer (automatic)', MONTHLYCHARGES=81.7),\n Row(title=u'Payback', GENDER=u'Male', SENIORCITIZEN=0, DEPENDENTS=0, TENURE=4, PAPERLESSBILLING=1, PAYMENTMETHOD=u'Electronic check', MONTHLYCHARGES=73.9)]"}, "execution_count": 31}], "cell_type": "code", "metadata": {}, "source": "\nimport ibmos2spark\n\n# @hidden_cell\ncredentials = {\n    'auth_url': 'https://identity.open.softlayer.com',\n    'project_id': '1d01a4b956544ae193baced39418dffc',\n    'region': 'dallas',\n    'user_id': '284703554b9c4314be591d5858cb32bb',\n    'username': 'member_d42cb5e9f21e150269239deba3605b754a22cff0',\n    'password': 'ao*9J2?vmvkO(HYe'\n}\n\nconfiguration_name = 'os_fd0d395ea9634e6ba8708dc2eebb3e12_configs'\nbmos = ibmos2spark.bluemix(sc, credentials, configuration_name)\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf_data_3 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option('inferSchema','true')\\\n  .load(bmos.url('MachineLearningProject', 'movies.csv'))\ndf_data_3.take(5)\n"}, {"metadata": {}, "cell_type": "markdown", "source": "Explore the loaded data by using the following Apache\u00ae Spark DataFrame methods:\n-  print schema\n-  count all the records\n-  print top five records"}, {"execution_count": 32, "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- title: string (nullable = true)\n |-- GENDER: string (nullable = true)\n |-- SENIORCITIZEN: integer (nullable = true)\n |-- DEPENDENTS: integer (nullable = true)\n |-- TENURE: integer (nullable = true)\n |-- PAPERLESSBILLING: integer (nullable = true)\n |-- PAYMENTMETHOD: string (nullable = true)\n |-- MONTHLYCHARGES: double (nullable = true)\n\n# of records: 1165\n"}], "cell_type": "code", "metadata": {}, "source": "df = df_data_3\n\ndf.printSchema()\nprint \"# of records: \" + str(df.count())"}, {"metadata": {}, "cell_type": "markdown", "source": "We can see that there are 1165 rows and we have 7 fields we will use to predict the title (label) of the movie."}, {"execution_count": 33, "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+------+-------------+----------+------+----------------+--------------------+--------------+\n|   title|GENDER|SENIORCITIZEN|DEPENDENTS|TENURE|PAPERLESSBILLING|       PAYMENTMETHOD|MONTHLYCHARGES|\n+--------+------+-------------+----------+------+----------------+--------------------+--------------+\n|Rounders|Female|            0|         1|    39|               1|Credit card (auto...|         99.95|\n|Rounders|Female|            0|         1|    71|               0|Bank transfer (au...|          20.1|\n|Rounders|Female|            0|         1|    72|               1|    Electronic check|         78.95|\n| Payback|  Male|            0|         0|    12|               0|Bank transfer (au...|          81.7|\n| Payback|  Male|            0|         0|     4|               1|    Electronic check|          73.9|\n+--------+------+-------------+----------+------+----------------+--------------------+--------------+\nonly showing top 5 rows\n\n"}], "cell_type": "code", "metadata": {}, "source": "df.show(5)"}, {"metadata": {}, "cell_type": "markdown", "source": "Top 5 rows"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"model\"></a>\n## 3. Create an Apache Spark machine learning model\n\nIn this section we will prepare data, create an Apache Spark machine learning pipeline, and train a model.\n\n\n### 3.1:  Prepare data\n\nIn this subsection we will split our data into: training, test, and predict datasets."}, {"execution_count": 34, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Training records: 808\nTest records: 340\nPrediction records: 17\n"}], "cell_type": "code", "metadata": {}, "source": "split_data = df.randomSplit([0.7, 0.28, 0.02], 24)\n\ntraining_data = split_data[0]\ntest_data = split_data[1]\npredict_data = split_data[2]\n\nprint \"Training records: \" + str(training_data.count())\nprint \"Test records: \" + str(test_data.count())\nprint \"Prediction records: \" + str(predict_data.count())"}, {"metadata": {}, "cell_type": "markdown", "source": "As you can see our data has been successfully split into three datasets: \n\n-  The training dataset, which is the largest group, is used for training.\n-  The test dataset will be used for model evaluation and is used to test the assumptions of the model.\n-  The predict dataset will be used for prediction."}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2:  Create pipeline and train a model\n\nIn this section we create an Apache Spark machine learning pipeline and then train the model.\n\nFirst we need to import several packages that will be used in the next few steps."}, {"execution_count": 35, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "First we need to convert all the string fields to numeric values."}, {"execution_count": 36, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "stringIndexer_labels = StringIndexer(inputCol=\"title\", outputCol=\"label\").fit(df)\nstringIndexer_gender = StringIndexer(inputCol=\"GENDER\", outputCol=\"GENDER_IX\").fit(df)\nstringIndexer_paymentmethod = StringIndexer(inputCol=\"PAYMENTMETHOD\", outputCol=\"PAYMENTMETHOD_IX\").fit(df)"}, {"metadata": {}, "cell_type": "markdown", "source": "Create a feature vector by combining all features together."}, {"execution_count": 37, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "vectorAssembler_features = VectorAssembler(inputCols=[\"GENDER_IX\",\"SENIORCITIZEN\",\"DEPENDENTS\",\"TENURE\",\"PAPERLESSBILLING\",\n                                                     \"PAYMENTMETHOD_IX\",\"MONTHLYCHARGES\"], outputCol=\"features\")"}, {"metadata": {}, "cell_type": "markdown", "source": "Next we define a Random Forest estimator."}, {"execution_count": 38, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"}, {"metadata": {}, "cell_type": "markdown", "source": "Next we convert the indexed labels back to the original label."}, {"execution_count": 39, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=stringIndexer_labels.labels)"}, {"metadata": {}, "cell_type": "markdown", "source": "Now we will put all the steps into a pipeline. "}, {"execution_count": 40, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "pipeline_rf = Pipeline(stages=[stringIndexer_labels,stringIndexer_gender, stringIndexer_paymentmethod,\n                               vectorAssembler_features, rf, labelConverter])"}, {"metadata": {}, "cell_type": "markdown", "source": "Now we will create a model using our pipeline and the training_data dataset."}, {"execution_count": 41, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "model_rf = pipeline_rf.fit(training_data)"}, {"metadata": {}, "cell_type": "markdown", "source": "Now we will check our model accuracy using our test_data dataset."}, {"execution_count": 42, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Accuracy = 0.473529\nTest Error = 0.526471\n"}], "cell_type": "code", "metadata": {}, "source": "predictions = model_rf.transform(test_data)\nevaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluatorRF.evaluate(predictions)\nprint(\"Accuracy = %g\" % accuracy)\nprint(\"Test Error = %g\" % (1.0 - accuracy))"}, {"metadata": {}, "cell_type": "markdown", "source": "At this point we would tune the model for desired accuracy, for this example we will move on."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"save\"></a>\n## 4. Persist model in IBM Watson Machine Learning"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you will learn how to store your pipeline and model in Watson Machine Learning repository by using python client libraries.\n\nFirst, you must import client libraries.\n\n**Note**: Apache Spark 2.0 or higher is required."}, {"execution_count": 43, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "from repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact"}, {"metadata": {}, "cell_type": "markdown", "source": "Authenticate to Watson Machine Learning service on Bluemix.\n\n**Action**: Use your Watson Machine Learning service instance credentials below.\n\n"}, {"execution_count": 16, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "username = '<from your watson ml service>'\npassword = '<from your watson ml service>'\nservice_path = '<from your watson ml service>'\ninstance_id = '<from your watson ml service>'\n"}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip**: service_path, user and password can be found on **Service Credentials** tab of service instance created in Bluemix. If you cannot see **instance_id** field in **Serice Credentials** generate new credentials by pressing **New credential (+)** button. "}, {"execution_count": 17, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "ml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)"}, {"metadata": {}, "cell_type": "markdown", "source": "Create model artifact (abstraction layer)"}, {"execution_count": 18, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "model_artifact = MLRepositoryArtifact(model_rf, training_data=training_data, name=\"Movie Prediction with Python\")"}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip**: The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service)."}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1: Save pipeline and model"}, {"execution_count": 19, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "saved_model = ml_repository_client.models.save(model_artifact)"}, {"metadata": {}, "cell_type": "markdown", "source": "Get saved model metadata from Watson Machine Learning using the meta.available_props() method."}, {"execution_count": 20, "outputs": [{"output_type": "execute_result", "execution_count": 20, "data": {"text/plain": "['inputDataSchema',\n 'evaluationMetrics',\n 'pipelineVersionHref',\n 'modelVersionHref',\n 'trainingDataRef',\n 'pipelineType',\n 'creationTime',\n 'lastUpdated',\n 'label',\n 'authorEmail',\n 'trainingDataSchema',\n 'authorName',\n 'version',\n 'modelType',\n 'runtime',\n 'evaluationMethod']"}, "metadata": {}}], "cell_type": "code", "metadata": {}, "source": "saved_model.meta.available_props()"}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip**:  **modelVersionHref** is our model unique id in Watson Machine Learning."}, {"execution_count": 21, "outputs": [{"output_type": "stream", "name": "stdout", "text": "https://ibm-watson-ml.mybluemix.net/v2/artifacts/models/44453df1-69c4-40cd-909c-bc7b5f3262a6/versions/c1acb55b-7223-4bd8-82b6-c408b6df6e3e\n"}], "cell_type": "code", "metadata": {}, "source": "print saved_model.meta.prop(\"modelVersionHref\")"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.2: Load model\n\nNow that we saved the model we will load it and verify the name."}, {"execution_count": 22, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)"}, {"execution_count": 23, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Movie Prediction with Python\n"}], "cell_type": "code", "metadata": {}, "source": "print str(loadedModelArtifact.name)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"predict\"></a>\n## 5. Predict locally and visualize\n\nIn this section we will score test data using the loaded model."}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.1: Make local prediction using loaded model and predict data"}, {"execution_count": 24, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "predictions = loadedModelArtifact.model_instance().transform(predict_data)"}, {"execution_count": 25, "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+------+-------------+----------+------+----------------+--------------------+--------------+-----+---------+----------------+--------------------+--------------------+--------------------+----------+--------------+\n|     title|GENDER|SENIORCITIZEN|DEPENDENTS|TENURE|PAPERLESSBILLING|       PAYMENTMETHOD|MONTHLYCHARGES|label|GENDER_IX|PAYMENTMETHOD_IX|            features|       rawPrediction|         probability|prediction|predictedLabel|\n+----------+------+-------------+----------+------+----------------+--------------------+--------------+-----+---------+----------------+--------------------+--------------------+--------------------+----------+--------------+\n|Inside Job|  Male|            0|         1|    28|               1|Bank transfer (au...|          74.9|  9.0|      0.0|             1.0|[0.0,0.0,1.0,28.0...|[0.11428571428571...|[0.00571428571428...|       9.0|    Inside Job|\n|Inside Job|  Male|            0|         1|    53|               1|        Mailed check|         78.75|  9.0|      0.0|             2.0|[0.0,0.0,1.0,53.0...|[0.33119747899159...|[0.01655987394957...|       9.0|    Inside Job|\n|    Narcos|  Male|            0|         0|    26|               1|Bank transfer (au...|          19.6| 10.0|      0.0|             1.0|[0.0,0.0,0.0,26.0...|[0.36184481446072...|[0.01809224072303...|       6.0|        Nomads|\n+----------+------+-------------+----------+------+----------------+--------------------+--------------+-----+---------+----------------+--------------------+--------------------+--------------------+----------+--------------+\nonly showing top 3 rows\n\n"}], "cell_type": "code", "metadata": {}, "source": "predictions.show(3)"}, {"execution_count": 26, "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-----+\n|      predictedLabel|count|\n+--------------------+-----+\n|           Spotlight|    1|\n|              Nomads|    2|\n|             RoboCop|    2|\n|    Punch-Drunk Love|    2|\n|            Pit Stop|    1|\n|          Rocket Men|    2|\n|          Inside Job|    3|\n|Prisoners of the Sun|    1|\n|        Paid in Full|    3|\n+--------------------+-----+\n\n"}], "cell_type": "code", "metadata": {}, "source": "predictions.select(\"predictedLabel\").groupBy(\"predictedLabel\").count().show()"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"deploy\"></a>\n## 6. Deploy and create online scoring endpoint"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you will learn how to create online scoring and to score a new data record by using the Watson Machine Learning REST API. \nFor more information about REST APIs, see the [Swagger Documentation](http://watson-ml-api.mybluemix.net/)."}, {"metadata": {}, "cell_type": "markdown", "source": "To work with the Watson Machine Leraning REST API you must generate an access token. To do that you can use the following sample code:"}, {"execution_count": 27, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "import urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{}:{}'.format(username, password))\nurl = '{}/v3/identity/token'.format(service_path)\nresponse = requests.get(url, headers=headers)\nmltoken = json.loads(response.text).get('token')"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "Now that we have the token we can create an online scoring endpoint.\n\nFirst we will check the model for existing deployments and get the deployments url, then we will create the online deployment."}, {"execution_count": 28, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Existing deployment count: 0\nhttps://ibm-watson-ml.mybluemix.net/v3/wml_instances/40cda31c-7686-4b23-946c-bd2d5bf7fab3/published_models/44453df1-69c4-40cd-909c-bc7b5f3262a6/deployments\n"}], "cell_type": "code", "metadata": {}, "source": "published_model_details = service_path + \"/v3/wml_instances/\" + instance_id + \"/published_models/\"\\\n+ loadedModelArtifact.uid \nheader = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n\nresponse_get_model_details = requests.get(published_model_details, headers=header)\n\nprint 'Existing deployment count: ' + str(json.loads(response_get_model_details.text).get('entity').get('deployments').get('count'))\ndeployments_endpoint = json.loads(response_get_model_details.text).get('entity').get('deployments').get('url')\nprint deployments_endpoint"}, {"execution_count": 29, "outputs": [{"output_type": "stream", "name": "stdout", "text": "https://ibm-watson-ml.mybluemix.net/v3/wml_instances/40cda31c-7686-4b23-946c-bd2d5bf7fab3/published_models/44453df1-69c4-40cd-909c-bc7b5f3262a6/deployments/7d53c96f-c277-47d7-a186-3e8adc62ad97/online\n"}], "cell_type": "code", "metadata": {}, "source": "payload_online_endpoint = {\"name\": \"Movie Prediction Deployment\", \"description\": \"Movie prediction endpoint\\\nfor suggesting movies to customers.\", \"type\": \"online\"}\nresponse_online = requests.post(deployments_endpoint, json=payload_online_endpoint, headers=header)\n\nscoring_endpoint = json.loads(response_online.text).get('entity').get('scoring_url')\nprint scoring_endpoint"}, {"metadata": {}, "cell_type": "markdown", "source": "Now we can send (POST) a new scoring request to our deployed model to get a movie prediction."}, {"execution_count": 30, "outputs": [{"output_type": "stream", "name": "stdout", "text": "An American Werewolf in London\n"}], "cell_type": "code", "metadata": {}, "source": "payload_scoring = {\"fields\": [\"GENDER\",\"SENIORCITIZEN\",\"DEPENDENTS\",\"TENURE\",\"PAPERLESSBILLING\",\\\n                             \"PAYMENTMETHOD\", \"MONTHLYCHARGES\"],\"values\": [[\"Male\",0,0,25,1,\"Credit card (automatic)\",79.95]]}\nresponse_scoring = requests.post(scoring_endpoint, json=payload_scoring, headers=header)\n\nprint json.loads(response_scoring.text)[\"values\"][0]\\\n[len(json.loads(response_scoring.text)[\"values\"][0])-1]\n"}, {"metadata": {}, "cell_type": "markdown", "source": "**Now we have a working online endpoint to use in our call center application**"}], "nbformat": 4}